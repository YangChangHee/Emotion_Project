{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cardiac-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "from copy import deepcopy\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cardiovascular-tackle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "private_labels=np.load(\"Nohot_private_label.npy\")\n",
    "private_pixels=np.load(\"private_pixels.npy\")\n",
    "public_labels=np.load(\"Nohot_public_label.npy\")\n",
    "public_pixels=np.load(\"public_pixels.npy\")\n",
    "train_labels=np.load(\"Nohot_train_label.npy\")\n",
    "train_pixels=np.load(\"train_pixels.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "viral-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , y_train = train_pixels,train_labels\n",
    "X_valid , y_valid = public_pixels,public_labels\n",
    "X_test , y_test = private_pixels,private_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "coordinated-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vals = np.mean(X_train, axis=0)\n",
    "std_val = np.std(X_train)\n",
    "\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_valid_centered = (X_valid - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afraid-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "input_data=torch.FloatTensor(X_train_centered)\n",
    "label=torch.LongTensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cheap-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data1=torch.FloatTensor(X_valid_centered)\n",
    "label1=torch.LongTensor(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "instructional-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data2=torch.FloatTensor(X_test_centered)\n",
    "label2=torch.LongTensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "regulation-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=input_data.transpose(1,3)\n",
    "input_data1=input_data1.transpose(1,3)\n",
    "input_data2=input_data2.transpose(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "organic-flood",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "trainset = TensorDataset(input_data,label)\n",
    "valset = TensorDataset(input_data1,label1)\n",
    "testset = TensorDataset(input_data2,label2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "herbal-dominican",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition={'train':trainset,'val':valset,'test':testset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dramatic-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "potential-while",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_planes, kernel_1_x, \n",
    "                 kernel_3_in, kernel_3_x, kernel_5_in, kernel_5_x, pool_planes):\n",
    "        super(Inception, self).__init__()\n",
    "        \n",
    "        ############### 1x1 conv brance ########################\n",
    "        # nn,Conv2d(in_dim, out_Dim, filter) stride=1 padding=1\n",
    "        self.b1 =nn.Sequential(\n",
    "        nn.Conv2d(in_planes, kernel_1_x, kernel_size=1),\n",
    "        nn.BatchNorm2d(kernel_1_x),\n",
    "        nn.ReLU(True),\n",
    "        )\n",
    "        ############### 1x1 conv => 3x3 conv brance ########################\n",
    "        self.b2 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes,kernel_3_in,kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_3_in),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(kernel_3_in,kernel_3_x,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(kernel_3_x),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        ############### 1x1 conv => 3x3 conv => 3x3 conv brance ############\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv2d(in_planes, kernel_5_in,kernel_size=1),\n",
    "            nn.BatchNorm2d(kernel_5_in),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(kernel_5_in,kernel_5_x,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(kernel_5_x),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(kernel_5_x,kernel_5_x,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(kernel_5_x),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.b4=nn.Sequential(\n",
    "            nn.MaxPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_planes),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        y1=self.b1(x) #2 64 227 227\n",
    "        y2=self.b2(x) #2 128 227 227 => y1+y2= 2 192 227 227\n",
    "        y3=self.b3(x) #2 32 227 227 => y1+y2+y3 = 2 224 227 227\n",
    "        y4=self.b4(x) #2 32 227 227 => 2 256 227 227\n",
    "        return torch.cat([y1,y2,y3,y4],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "empty-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GoogleNet,self).__init__()\n",
    "        self.pre_layers= nn.Sequential(\n",
    "            nn.Conv2d(1,192,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.a3= Inception(192, 64, 96, 128, 16, 32, 32) # 64 128 32 32 = 256\n",
    "        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64) # 128 192 96 64 = 480\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(3,stride=2,padding=1)\n",
    "        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)# 192 208 48 64 = 512\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
    "        self.linear = nn.Linear(25600, 7)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x= self.pre_layers(x)\n",
    "        x= self.a3(x)       # 2 256 48 48\n",
    "        x= self.b3(x)       # 2 480 48 48\n",
    "        x= self.max_pool(x) # 2 480 24 24\n",
    "        x = self.a4(x)      # 2 512 24 24\n",
    "        x = self.b4(x)      # 2 512 24 24\n",
    "        x = self.c4(x)      # 2 512 24 24\n",
    "        x = self.d4(x)      # 2 528 24 24\n",
    "        x = self.e4(x)      # 2 832 24 24\n",
    "        x = self.max_pool(x)# 2 832 12 12\n",
    "        x = self.a5(x)      # 2 832 12 12\n",
    "        x = self.b5(x)      # 2 1024 12 12\n",
    "        x = self.avgpool(x) # 2 1024 6 6\n",
    "        x = x.view(x.size(0), -1) # 36864\n",
    "        x = self.linear(x)  # 10\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rapid-culture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimension_check():\n",
    "    net=GoogleNet()\n",
    "    x= torch.randn(1,1,48,48)\n",
    "    y=net(x)\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "multiple-occasion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "dimension_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "optional-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,partition,optimizer,criterion):\n",
    "    trainloader = torch.utils.data.DataLoader(partition['train'],\n",
    "                                             batch_size=64,\n",
    "                                             shuffle=True)\n",
    "    net.train()\n",
    "    correct = 0\n",
    "    total =0\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(tqdm(trainloader, 0)):\n",
    "        time.sleep(0.0000001)\n",
    "        optimizer.zero_grad()\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        outputs = net(inputs)\n",
    "        #break\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        #if i % 200 == 0:\n",
    "        #    print(correct,train_loss)\n",
    "    train_loss = train_loss / len(trainloader)\n",
    "    train_acc = 100 * correct / total\n",
    "    return net, train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "medium-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, partition):\n",
    "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
    "                                             batch_size=64, \n",
    "                                             shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(testloader):\n",
    "            time.sleep(0.0000001)\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "            outputs = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        test_acc = 100 * correct / total\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "perfect-jaguar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, partition, criterion):\n",
    "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
    "                                            batch_size=64, \n",
    "                                            shuffle=False, num_workers=2)\n",
    "    net.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(valloader):\n",
    "            time.sleep(0.0000001)\n",
    "            images, labels = data\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            outputs = net(images)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(valloader)\n",
    "        val_acc = 100 * correct / total\n",
    "    return val_loss, val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "promising-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "from tqdm import trange\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inappropriate-magic",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:47<00:00,  4.17it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.94it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 111.60, train_loss : 1.85, train_acc : 26.39,val_acc : 18.61,val_loss : 2.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:49<00:00,  4.11it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.90it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 113.04, train_loss : 1.59, train_acc : 36.37,val_acc : 41.71,val_loss : 1.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:49<00:00,  4.08it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.78it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 113.85, train_loss : 1.37, train_acc : 46.77,val_acc : 49.43,val_loss : 1.30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.07it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.79it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.17, train_loss : 1.23, train_acc : 52.64,val_acc : 51.57,val_loss : 1.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.07it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.80it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.21, train_loss : 1.15, train_acc : 55.95,val_acc : 53.44,val_loss : 1.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.07it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.78it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.31, train_loss : 1.09, train_acc : 58.30,val_acc : 55.48,val_loss : 1.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.81it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.33, train_loss : 1.04, train_acc : 60.16,val_acc : 56.48,val_loss : 1.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.81it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.34, train_loss : 1.00, train_acc : 62.27,val_acc : 56.34,val_loss : 1.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.76it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.46, train_loss : 0.96, train_acc : 63.73,val_acc : 57.56,val_loss : 1.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.76it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.46, train_loss : 0.92, train_acc : 65.68,val_acc : 57.93,val_loss : 1.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.77it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.45, train_loss : 0.86, train_acc : 67.69,val_acc : 59.10,val_loss : 1.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.76it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.48, train_loss : 0.82, train_acc : 69.31,val_acc : 52.08,val_loss : 1.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.80it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.47, train_loss : 0.76, train_acc : 71.52,val_acc : 60.82,val_loss : 1.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.70it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.51, train_loss : 0.70, train_acc : 73.89,val_acc : 59.68,val_loss : 1.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.81it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.47, train_loss : 0.63, train_acc : 76.64,val_acc : 59.10,val_loss : 1.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.82it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.48, train_loss : 0.57, train_acc : 78.68,val_acc : 56.84,val_loss : 1.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.83it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.36, train_loss : 0.51, train_acc : 81.30,val_acc : 57.59,val_loss : 1.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.80it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.40, train_loss : 0.43, train_acc : 83.85,val_acc : 60.46,val_loss : 1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.76it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.46, train_loss : 0.38, train_acc : 86.06,val_acc : 57.68,val_loss : 1.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.75it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.45, train_loss : 0.32, train_acc : 88.29,val_acc : 58.26,val_loss : 1.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.78it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.43, train_loss : 0.28, train_acc : 90.04,val_acc : 58.93,val_loss : 1.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 449/449 [01:50<00:00,  4.06it/s]\n",
      "100%|██████████| 57/57 [00:03<00:00, 14.75it/s]\n",
      "  0%|          | 0/449 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 114.47, train_loss : 0.24, train_acc : 91.14,val_acc : 59.46,val_loss : 2.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 125/449 [00:30<01:19,  4.07it/s]"
     ]
    }
   ],
   "source": [
    "seed = 123\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "net=GoogleNet()\n",
    "net.cuda()\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0015, weight_decay=0.00001)\n",
    "train_losses=[]\n",
    "val_losses=[]\n",
    "train_accs=[]\n",
    "val_accs=[]\n",
    "for epoch in range(100):\n",
    "    ts=time.time()\n",
    "    net, train_loss,train_acc=train(net, partition, optimizer, criterion)\n",
    "    val_loss, val_acc = validate(net, partition, criterion)\n",
    "    te=time.time()\n",
    "    print('time : {:2.2f}, train_loss : {:2.2f}, train_acc : {:2.2f},val_acc : {:2.2f},val_loss : {:2.2f}'.format(te-ts,train_loss,train_acc,val_acc,val_loss))\n",
    "    train_loss=round(train_loss,2)\n",
    "    val_loss=round(val_loss,2)\n",
    "    train_acc=round(train_acc,2)\n",
    "    val_acc=round(val_acc,2)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "test_acc = test(net,partition)\n",
    "test_acc=round(test_acc,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/home/lab/양창희/Emotion Project/fer2013/result_CNN_pytorch/Gogle_train_loss.npy\",train_loss)\n",
    "np.save(\"/home/lab/양창희/Emotion Project/fer2013/result_CNN_pytorch/Gogle_val_loss.npy\",val_loss)\n",
    "np.save(\"/home/lab/양창희/Emotion Project/fer2013/result_CNN_pytorch/Gogle_train_acc.npy\",train_acc)\n",
    "np.save(\"/home/lab/양창희/Emotion Project/fer2013/result_CNN_pytorch/Gogle_val_acc.npy\",val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-simulation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
